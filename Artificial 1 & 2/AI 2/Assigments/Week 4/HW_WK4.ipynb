{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Markov Decision Processes (Chapter 17)\n",
    "First we define an MDP, and the special case of a GridMDP, in which\n",
    "states are laid out in a 2-dimensional grid. We also represent a policy\n",
    "as a dictionary of {state: action} pairs, and a Utility function as a\n",
    "dictionary of {state: number} pairs. We then define the value_iteration\n",
    "and policy_iteration algorithms.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import vector_add, orientations, turn_right, turn_left\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state. [Page 646]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states or self.get_states_from_transitions(transitions)\n",
    "\n",
    "        self.init = init\n",
    "\n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "\n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions or {}\n",
    "        if not self.transitions:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "\n",
    "        # self.check_consistency()\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "\n",
    "        if not self.transitions:\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set(tr[1] for actions in transitions.values()\n",
    "                     for effects in actions.values()\n",
    "                     for tr in effects)\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "\n",
    "        # check reward for each state\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "\n",
    "        # check that all terminals are valid states\n",
    "        assert all(t in self.states for t in self.terminals)\n",
    "\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP2(MDP):\n",
    "    \"\"\"\n",
    "    Inherits from MDP. Handles terminal states, and transitions to and from terminal states better.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions, reward=None, gamma=0.9):\n",
    "        MDP.__init__(self, init, actlist, terminals, transitions, reward, gamma=gamma)\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return self.transitions[state][action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMDP(MDP):\n",
    "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1]. All you have to do is\n",
    "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
    "    (unreachable state). Also, you should specify the terminal states.\n",
    "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        grid.reverse()  # because we want row 0 on bottom, not on top\n",
    "        reward = {}\n",
    "        states = set()\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        self.grid = grid\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                if grid[y][x]:\n",
    "                    states.add((x, y))\n",
    "                    reward[(x, y)] = grid[y][x]\n",
    "        self.states = states\n",
    "        actlist = orientations\n",
    "        transitions = {}\n",
    "        for s in states:\n",
    "            transitions[s] = {}\n",
    "            for a in actlist:\n",
    "                transitions[s][a] = self.calculate_T(s, a)\n",
    "        MDP.__init__(self, init, actlist=actlist,\n",
    "                     terminals=terminals, transitions=transitions,\n",
    "                     reward=reward, states=states, gamma=gamma)\n",
    "\n",
    "    def calculate_T(self, state, action):\n",
    "        if action:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state, turn_right(action))),\n",
    "                    (0.1, self.go(state, turn_left(action)))]\n",
    "        else:\n",
    "            return [(0.0, state)]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        return self.transitions[state][action] if action else [(0.0, state)]\n",
    "\n",
    "    def go(self, state, direction):\n",
    "        \"\"\"Return the state that results from going in this direction.\"\"\"\n",
    "\n",
    "        state1 = vector_add(state, direction)\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
    "\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n",
    "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "\n",
    "\n",
    "\"\"\" [Figure 17.1]\n",
    "A 4x3 grid environment that presents the agent with a sequential decision problem.\n",
    "\"\"\"\n",
    "\n",
    "sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "                                           [-0.04, None, -0.04, -1],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04]],\n",
    "                                          terminals=[(3, 2), (3, 1)])\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "\n",
    "\n",
    "def value_iteration(mdp, epsilon=0.001):\n",
    "    \"\"\"Solving an MDP by value iteration. [Figure 17.4]\"\"\"\n",
    "\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max(sum(p * U[s1] for (p, s1) in T(s, a))\n",
    "                                       for a in mdp.actions(s))\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta <= epsilon * (1 - gamma) / gamma:\n",
    "            return U\n",
    "\n",
    "\n",
    "def best_policy(mdp, U):\n",
    "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
    "    as a mapping from state to action. [Equation 17.4]\"\"\"\n",
    "\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        pi[s] = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "    return pi\n",
    "\n",
    "\n",
    "def expected_utility(a, s, U, mdp):\n",
    "    \"\"\"The expected utility of doing a in state s, according to the MDP and U.\"\"\"\n",
    "\n",
    "    return sum(p * U[s1] for (p, s1) in mdp.T(s, a))\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    \"\"\"Solve an MDP by policy iteration [Figure 17.7]\"\"\"\n",
    "\n",
    "    U = {s: 0 for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            a = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            print(\"Computed Utilities\")\n",
    "            grid_U = mdp.to_grid(U)\n",
    "            #print(grid_U)\n",
    "            for entry in grid_U:\n",
    "                print(entry)\n",
    "            print('\\nOptimal Policy')   \n",
    "            print(pi)\n",
    "            return pi\n",
    "        \n",
    "def policy_iteration_2(mdp):\n",
    "    \"\"\"Solve an MDP by policy iteration [Figure 17.7]\"\"\"\n",
    "\n",
    "    U = {s: 0 for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            a = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            print(\"Computed Utilities\")\n",
    "            grid_U = mdp.to_grid(U)\n",
    "            #print(grid_U)\n",
    "            for entry in grid_U:\n",
    "                print(entry)\n",
    "            print('\\nOptimal Policy')   \n",
    "            print(pi)\n",
    "            return pi, np.array(grid_U)\n",
    "\n",
    "\n",
    "def policy_evaluation(pi, U, mdp, k=20):\n",
    "    \"\"\"Return an updated utility mapping U from each state in the MDP to its\n",
    "    utility, using an approximation (modified policy iteration).\"\"\"\n",
    "\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            U[s] = R(s) + gamma * sum(p * U[s1] for (p, s1) in T(s, pi[s]))\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states=[(3, 2), (3, 1)]\n",
    "mdp_env = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "                   [-0.04, None, -0.04, -1],\n",
    "                   [-0.04, -0.04, -0.04, -0.04]],\n",
    "                   terminals=terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Utilities\n",
      "[0.5094155954147008, 0.649586359613105, 0.7953622428927029, 1.0]\n",
      "[0.3985112545104691, None, 0.4864404559151057, -1.0]\n",
      "[0.2964665410943774, 0.2539605460927299, 0.34478839971672015, 0.12994247010553683]\n",
      "\n",
      "Optimal Policy\n",
      "{(0, 1): (0, 1), (1, 2): (1, 0), (2, 1): (0, 1), (0, 0): (0, 1), (3, 1): None, (2, 0): (0, 1), (3, 0): (-1, 0), (0, 2): (1, 0), (2, 2): (1, 0), (1, 0): (1, 0), (3, 2): None}\n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   >      ^   <\n"
     ]
    }
   ],
   "source": [
    "from utils import print_table\n",
    "\n",
    "print_table(mdp_env.to_arrows(policy_iteration(mdp_env)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_expected_utility(mdp, policy, state, terminals):\n",
    "    dirs = {\"up\": (0,1),\n",
    "            \"right\": (1,0),\n",
    "            \"left\": (-1,0)}\n",
    "    avg_util = []\n",
    "    \n",
    "    for i in range(100000):\n",
    "        curr_state = state\n",
    "        discount = mdp.gamma\n",
    "        cum_util = mdp.R(curr_state)\n",
    "        counter = 0\n",
    "        #print(curr_state)\n",
    "        while curr_state not in terminals:\n",
    "            check_dir = policy[curr_state]\n",
    "            next_move = random.choices(['up', 'left', 'right'], weights=[0.8, 0.1, 0.1])[0]\n",
    "            #print(next_move)\n",
    "            counter += 1\n",
    "\n",
    "            if check_dir == (0,1): # north\n",
    "                curr_state = mdp.go(curr_state, dirs[next_move])\n",
    "\n",
    "            elif check_dir == (1,0):  # west\n",
    "                curr_state = mdp.go(curr_state, turn_right(dirs[next_move]))\n",
    "                \n",
    "            elif check_dir == (0,-1):  # south\n",
    "                curr_state = mdp.go(curr_state, turn_right(turn_right(dirs[next_move])))\n",
    "\n",
    "            else:   # east\n",
    "                curr_state = mdp.go(curr_state, turn_left(dirs[next_move]))\n",
    "\n",
    "            cum_util += (discount**counter) * mdp.R(curr_state)\n",
    "        avg_util.append(cum_util)\n",
    "        \n",
    "    return sum(avg_util)/len(avg_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Utilities\n",
      "[0.5094155954147008, 0.649586359613105, 0.7953622428927029, 1.0]\n",
      "[0.3985112545104691, None, 0.4864404559151057, -1.0]\n",
      "[0.29646654109402815, 0.25396054609199764, 0.34478839971636, 0.12994247010515378]\n",
      "\n",
      "Optimal Policy\n",
      "{(0, 1): (0, 1), (1, 2): (1, 0), (2, 1): (0, 1), (0, 0): (0, 1), (3, 1): None, (2, 0): (0, 1), (3, 0): (-1, 0), (0, 2): (1, 0), (2, 2): (1, 0), (1, 0): (1, 0), (3, 2): None}\n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   >      ^   <\n"
     ]
    }
   ],
   "source": [
    "from utils import print_table\n",
    "\n",
    "print_table(mdp_env.to_arrows(policy_iteration(mdp_env)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Utilities\n",
      "[0.5094155954147008, 0.649586359613105, 0.7953622428927029, 1.0]\n",
      "[0.3985112545104691, None, 0.4864404559151057, -1.0]\n",
      "[0.29646654109402715, 0.2539605460919955, 0.34478839971635894, 0.12994247010515267]\n",
      "\n",
      "Optimal Policy\n",
      "{(0, 1): (0, 1), (1, 2): (1, 0), (2, 1): (0, 1), (0, 0): (0, 1), (3, 1): None, (2, 0): (0, 1), (3, 0): (-1, 0), (0, 2): (1, 0), (2, 2): (1, 0), (1, 0): (1, 0), (3, 2): None}\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimal_policy, exp_utils_1 = policy_iteration_2(mdp_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7950932381934493"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "eval_expected_utility(mdp_env, optimal_policy, (2,2), terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_env_1_times = [3.78, 3.08, 2.57, 3.02, 3.51, 1.49, 2.66, 1.74, 0.967]\n",
    "mdp_env_1_utils = [[0.5093, 0.6497, 0.7945, 1.0],\n",
    "                   [0.3979, None, 0.4840, -1.0],\n",
    "                   [0.2958, 0.2546, 0.3456, 0.1268]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_env_1_times = np.array(mdp_env_1_times)\n",
    "mdp_env_1_utils = np.array(mdp_env_1_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states_2 =[(4,2), (1,0)]\n",
    "mdp_env_2 = GridMDP([[-0.04, -0.04, -0.04, -0.04, -1],\n",
    "                   [-0.04, None, -0.04, -0.04, -0.04],\n",
    "                   [-0.04, +1, -0.04, None, -0.04]],\n",
    "                   terminals=terminal_states_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Utilities\n",
      "[0.5228585047669859, 0.4103147846734511, 0.49989161677304544, 0.4018786173926277, -1.0]\n",
      "[0.6651012621074259, None, 0.6483252924828785, 0.5087508639030285, 0.25180946775265667]\n",
      "[0.8130319929556795, 1.0, 0.8113728311246183, None, 0.1723205082497349]\n",
      "\n",
      "Optimal Policy\n",
      "{(0, 1): (0, -1), (1, 2): (-1, 0), (4, 0): (0, 1), (2, 1): (0, -1), (0, 0): (1, 0), (3, 1): (-1, 0), (2, 0): (-1, 0), (4, 2): None, (0, 2): (0, -1), (2, 2): (0, -1), (1, 0): None, (3, 2): (-1, 0), (4, 1): (-1, 0)}\n",
      "v   <      v   <      .\n",
      "v   None   v   <      <\n",
      ">   .      <   None   ^\n"
     ]
    }
   ],
   "source": [
    "from utils import print_table\n",
    "\n",
    "print_table(mdp_env_2.to_arrows(policy_iteration(mdp_env_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Utilities\n",
      "[0.5228585047669859, 0.4103147846734511, 0.49989161677304544, 0.4018786173926277, -1.0]\n",
      "[0.6651012621074259, None, 0.6483252924828785, 0.5087508639030285, 0.251809467752548]\n",
      "[0.8130319929556795, 1.0, 0.8113728311246183, None, 0.17232050824852732]\n",
      "\n",
      "Optimal Policy\n",
      "{(0, 1): (0, -1), (1, 2): (-1, 0), (4, 0): (0, 1), (2, 1): (0, -1), (0, 0): (1, 0), (3, 1): (-1, 0), (2, 0): (-1, 0), (4, 2): None, (0, 2): (0, -1), (2, 2): (0, -1), (1, 0): None, (3, 2): (-1, 0), (4, 1): (-1, 0)}\n",
      "Wall time: 4.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimal_policy_2 = policy_iteration(mdp_env_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 89.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "eval_expected_utility(mdp_env_2, optimal_policy_2, (4,2), terminal_states_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_env_2_times = [0.929, 0.937, 1.74, 4.05, 1.71, 1.87, 2.93, 3.19, 2.95, 3.58, 3.37, 3.71]\n",
    "mdp_env_2_utils = [[0.5230, 0.4101, 0.4997, 0.4014, -1.0],\n",
    "                   [0.6652, None, 0.6478, 0.5085, 0.2523],\n",
    "                   [0.8132, 1.0, 0.8118, None, 0.1732]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.535222222222222\n",
      "2.5805000000000002\n"
     ]
    }
   ],
   "source": [
    "mdp_1_mean = np.mean(mdp_env_1_times)\n",
    "mdp_2_mean = np.mean(mdp_env_2_times)\n",
    "print(mdp_1_mean)\n",
    "print(mdp_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7574814814814813\n"
     ]
    }
   ],
   "source": [
    "mdp_env_2_times = np.array(mdp_env_2_times)\n",
    "\n",
    "avg_dev_1 = np.sum(np.abs(mdp_env_1_times - np.mean(mdp_env_1_times))) / len(mdp_env_1_times)\n",
    "print(avg_dev_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95275\n"
     ]
    }
   ],
   "source": [
    "avg_dev_2 = np.sum(np.abs(mdp_env_2_times - np.mean(mdp_env_2_times))) / len(mdp_env_2_times)\n",
    "print(avg_dev_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states_3 =[(4,7), (1,5), (3,1)]\n",
    "mdp_env_3 = GridMDP([[-0.04, -0.04, -0.04, -0.04, -1],\n",
    "                   [-0.04, None, -0.04, -0.04, -0.04],\n",
    "                   [-0.04, +1, -0.04, None, -0.04],\n",
    "                   [-0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                   [-0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                   [-0.04, -0.04, -0.04, -0.04, -0.04],\n",
    "                   [-0.04, -0.04, -0.04, +1, -0.04],\n",
    "                   [-0.04, -0.04, -0.04, -0.04, -0.04]],\n",
    "                   terminals=terminal_states_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Utilities\n",
      "[0.5120066558005355, 0.40078633192242147, 0.48870133423964374, 0.39190178116992014, -1.0]\n",
      "[0.6525767873687076, None, 0.6352213945296291, 0.49739622457870963, 0.2740118945898884]\n",
      "[0.7987680078365839, 1.0, 0.7962302900137203, None, 0.3397016164787702]\n",
      "[0.6670677441488915, 0.7990964614794538, 0.6562262722894842, 0.5379537878049192, 0.4424379521008215]\n",
      "[0.5465125567625505, 0.6337516762968789, 0.5468454880338068, 0.6339447872671986, 0.5475037437629233]\n",
      "[0.4475700951760174, 0.5468454880338068, 0.6566011198792221, 0.7992407727854066, 0.6682963555141838]\n",
      "[0.4961690409227429, 0.6339447872671985, 0.7992407727854066, 1.0, 0.8003027971766953]\n",
      "[0.4383053947168699, 0.5475037437629232, 0.6682963555141838, 0.8003027971766953, 0.6684013908935419]\n",
      "\n",
      "Optimal Policy\n",
      "{(4, 0): (0, 1), (3, 4): (-1, 0), (4, 3): (0, -1), (3, 1): None, (3, 7): (-1, 0), (4, 6): (0, -1), (0, 2): (1, 0), (0, 5): (1, 0), (2, 2): (1, 0), (1, 0): (1, 0), (2, 5): (-1, 0), (1, 3): (0, 1), (4, 2): (0, -1), (3, 0): (0, 1), (4, 5): (0, -1), (3, 3): (0, -1), (3, 6): (-1, 0), (0, 1): (1, 0), (0, 7): (0, -1), (2, 4): (-1, 0), (1, 2): (1, 0), (0, 4): (0, 1), (2, 1): (1, 0), (2, 7): (0, -1), (1, 5): None, (3, 2): (0, -1), (4, 1): (-1, 0), (4, 7): None, (4, 4): (0, -1), (0, 0): (1, 0), (1, 1): (1, 0), (0, 3): (0, 1), (2, 0): (1, 0), (1, 4): (0, 1), (0, 6): (0, -1), (2, 3): (0, -1), (1, 7): (-1, 0), (2, 6): (0, -1)}\n",
      "Wall time: 16.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(4, 0): (0, 1),\n",
       " (3, 4): (-1, 0),\n",
       " (4, 3): (0, -1),\n",
       " (3, 1): None,\n",
       " (3, 7): (-1, 0),\n",
       " (4, 6): (0, -1),\n",
       " (0, 2): (1, 0),\n",
       " (0, 5): (1, 0),\n",
       " (2, 2): (1, 0),\n",
       " (1, 0): (1, 0),\n",
       " (2, 5): (-1, 0),\n",
       " (1, 3): (0, 1),\n",
       " (4, 2): (0, -1),\n",
       " (3, 0): (0, 1),\n",
       " (4, 5): (0, -1),\n",
       " (3, 3): (0, -1),\n",
       " (3, 6): (-1, 0),\n",
       " (0, 1): (1, 0),\n",
       " (0, 7): (0, -1),\n",
       " (2, 4): (-1, 0),\n",
       " (1, 2): (1, 0),\n",
       " (0, 4): (0, 1),\n",
       " (2, 1): (1, 0),\n",
       " (2, 7): (0, -1),\n",
       " (1, 5): None,\n",
       " (3, 2): (0, -1),\n",
       " (4, 1): (-1, 0),\n",
       " (4, 7): None,\n",
       " (4, 4): (0, -1),\n",
       " (0, 0): (1, 0),\n",
       " (1, 1): (1, 0),\n",
       " (0, 3): (0, 1),\n",
       " (2, 0): (1, 0),\n",
       " (1, 4): (0, 1),\n",
       " (0, 6): (0, -1),\n",
       " (2, 3): (0, -1),\n",
       " (1, 7): (-1, 0),\n",
       " (2, 6): (0, -1)}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "policy_iteration(mdp_env_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
