{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataset was gather from Kaggle\n",
    "\n",
    "Dataset name: Twitter Sentiment Analysis\n",
    "\n",
    "Link: https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech?select=train.csv\n",
    "\n",
    "Potential uses of the dataset, per website: \n",
    "\n",
    "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
    "\n",
    "Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "np.random.seed(47)\n",
    "\n",
    "tweet_train = pd.read_csv('train.csv')\n",
    "\n",
    "#running train dataset simitaneously to run prediction\n",
    "tweet_test = pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "tweet_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                              tweet\n",
      "31957      0  ate @user isz that youuu?ðððððð...\n",
      "31958      0    to see nina turner on the airwaves trying to...\n",
      "31959      0  listening to sad songs on a monday morning otw...\n",
      "31960      1  @user #sikh #temple vandalised in in #calgary,...\n",
      "31961      0                   thank you @user for you follow  \n"
     ]
    }
   ],
   "source": [
    "tweet_train = tweet_train.drop(['id'],axis=1)\n",
    "\n",
    "#dropping column id\n",
    "tweet_test = tweet_test.drop(['id'],axis=1)\n",
    "\n",
    "print(tweet_train.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train['tweet'] = tweet_train['tweet'].apply(preprocessor)\n",
    "\n",
    "#running preprocessor in test data\n",
    "tweet_test['tweet'] = tweet_test['tweet'].apply(preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy father s day user ð ð ð ð \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#checking if preprocessor works by looking at column 'tweet' and row 28. \n",
    "\n",
    "print(tweet_train.loc[28][1] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                              tweet\n",
      "17253      0   user getting spoilt or what wishing all dads ...\n",
      "15758      0                 user smile with my dog dogsarejoy \n",
      "25747      0   user user user user user user user user ð ð m...\n",
      "20548      0         i am successful i_am positive affirmation \n",
      "17073      0   duschszene fear origins pib moore temprano cl...\n"
     ]
    }
   ],
   "source": [
    "#randomizing out train dataset. \n",
    "\n",
    "tweet_train = tweet_train.reindex(np.random.permutation(tweet_train.index))\n",
    "\n",
    "\n",
    "\n",
    "print (tweet_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/izzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'y', 'z', 'user']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop = stop + [u'a',u'b',u'c',u'd',u'e',u'f',u'g',u'h',u'i',u'j',u'k',u'l',u'm',u'n',u'o',u'p',u'q',u'r',u's',u't',u'v',u'w',u'x',u'y',u'z']\n",
    "\n",
    "stop.append('user') #user is a common word in our dataset, therefore removing\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17253    [utf8, getting, spoilt, wishing, dad, amp, hea...\n",
       "15758                       [utf8, smile, dog, dogsarejoy]\n",
       "25747                           [utf8, ð, ð, monday, ð, ð]\n",
       "20548     [utf8i, successful, i_am, positive, affirmation]\n",
       "17073    [utf8, duschszene, fear, origin, pib, moore, t...\n",
       "                               ...                        \n",
       "23112    [utf8, strong, word, love, introduction, thoug...\n",
       "11528    [utf8, life, vanity, let, hand, find, still, a...\n",
       "14663    [utf8, clearwater, polar, bear, climb, racing,...\n",
       "18310                                 [utf8, kindð, rð, ð]\n",
       "5255     [utf8wishing, happy, day, daddy, ð, daddy, fat...\n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_into_lemmas(tweet_train):\n",
    "    tweet_train = 'utf8' + str(tweet_train).lower()\n",
    "    words = TextBlob(tweet_train).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words if word not in stop]\n",
    "\n",
    "#running lemmas into our test data to get it ready for our stasitstic\n",
    "tweet_test.tweet.apply(split_into_lemmas)\n",
    "\n",
    "tweet_train.tweet.apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41718\n",
      "CPU times: user 8.53 s, sys: 20.5 ms, total: 8.55 s\n",
      "Wall time: 8.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(tweet_train['tweet'])\n",
    "print (len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (31962, 41718)\n",
      "number of non-zeros: 276004\n",
      "sparsity: 0.02%\n",
      "CPU times: user 13 s, sys: 17.9 ms, total: 13 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "tweets_bow = bow_transformer.transform(tweet_train['tweet'])\n",
    "\n",
    "#running our bag of words transformer into out test dataset\n",
    "tweet_final_test = bow_transformer.transform(tweet_test['tweet'])\n",
    "\n",
    "print ('sparse matrix shape:', tweets_bow.shape)\n",
    "print ('number of non-zeros:', tweets_bow.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * tweets_bow.nnz / (tweets_bow.shape[0] * tweets_bow.shape[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15981, 41718)\n",
      "(15981, 41718)\n"
     ]
    }
   ],
   "source": [
    "#the train dataset has a told of 31,962 rows.  half of that is 15981\n",
    "\n",
    "tweets_bow_train = tweets_bow[:15981]\n",
    "tweets_bow_test = tweets_bow[15981:]\n",
    "tweets_class_train = tweet_train['label'][:15981]\n",
    "tweets_class_test = tweet_train['label'][15981:]\n",
    "\n",
    "print (tweets_bow_train.shape)\n",
    "print (tweets_bow_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "tweets_class = MultinomialNB().fit(tweets_bow_train, tweets_class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = tweets_class.predict(tweets_bow_test)\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9432451035604781\n",
      "confusion matrix\n",
      " [[14594   212]\n",
      " [  695   480]]\n",
      "(row=expected, col=predicted)\n"
     ]
    }
   ],
   "source": [
    "print ('accuracy', accuracy_score(tweets_class_test, predictions))\n",
    "print ('confusion matrix\\n', confusion_matrix(tweets_class_test, predictions))\n",
    "print ('(row=expected, col=predicted)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     14806\n",
      "           1       0.69      0.41      0.51      1175\n",
      "\n",
      "    accuracy                           0.94     15981\n",
      "   macro avg       0.82      0.70      0.74     15981\n",
      "weighted avg       0.94      0.94      0.94     15981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#running stats on train dataset\n",
    "\n",
    "print (classification_report(tweets_class_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9023840810963019\n",
      "confusion matrix\n",
      " [[14389   417]\n",
      " [ 1143    32]]\n",
      "(row=expected, col=predicted)\n"
     ]
    }
   ],
   "source": [
    "#running new prediction statistic with the test dataset\n",
    "\n",
    "predictions2 = tweets_class.predict(tweet_final_test[:15981])\n",
    "\n",
    "print ('accuracy', accuracy_score(tweets_class_test, predictions2))\n",
    "print ('confusion matrix\\n', confusion_matrix(tweets_class_test, predictions2))\n",
    "print ('(row=expected, col=predicted)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     14806\n",
      "           1       0.07      0.03      0.04      1175\n",
      "\n",
      "    accuracy                           0.90     15981\n",
      "   macro avg       0.50      0.50      0.49     15981\n",
      "weighted avg       0.86      0.90      0.88     15981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(tweets_class_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating model to test out any text.\n",
    "\n",
    "\n",
    "def predict_tweets(new_tweets): \n",
    "    new_sample = bow_transformer.transform([new_tweets])\n",
    "    print (new_tweets, np.around(tweets_class.predict_proba(new_sample), decimals=3),\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horrible. Terrible. Dreadful. Awful. Pile of garbage. Junk. [[0.04 0.96]] \n",
      "\n",
      "Fantastic. Amazing. Terrific. Classic. Best! Extraordinary. Authentic. Ideal. Vibrant. Powerful. Perfect. Imaginative. Incredible. Happy. Love. Pleasure. [[1. 0.]] \n",
      "\n",
      "Okay. Great. [[0.983 0.017]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_tweets('Horrible. Terrible. Dreadful. Awful. Pile of garbage. Junk.')\n",
    "predict_tweets('Fantastic. Amazing. Terrific. Classic. Best! Extraordinary. Authentic. Ideal. Vibrant. Powerful. Perfect. Imaginative. Incredible. Happy. Love. Pleasure.')\n",
    "predict_tweets('Okay. Great.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alright white supremacy [[0.166 0.834]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_tweets('alright white supremacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitesupremacy and hate jews [[0.255 0.745]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_tweets('whitesupremacy and hate jews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little fucking kikes. They get ruled by people like me. Little fucking octaroons. My ancestors fucking enslaved those little pieces of fucking shit. [[1. 0.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_tweets('Little fucking kikes. They get ruled by people like me. Little fucking octaroons. My ancestors fucking enslaved those little pieces of fucking shit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
